{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97bed655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-06 14:30:00+00:00</th>\n",
       "      <td>192.419998</td>\n",
       "      <td>194.759995</td>\n",
       "      <td>192.205002</td>\n",
       "      <td>194.449997</td>\n",
       "      <td>11260893</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-06 14:30:00+00:00</th>\n",
       "      <td>46.444000</td>\n",
       "      <td>47.387001</td>\n",
       "      <td>46.361000</td>\n",
       "      <td>47.214500</td>\n",
       "      <td>11433656</td>\n",
       "      <td>NVDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-06 14:30:00+00:00</th>\n",
       "      <td>145.289993</td>\n",
       "      <td>147.850006</td>\n",
       "      <td>145.089996</td>\n",
       "      <td>147.580002</td>\n",
       "      <td>10481937</td>\n",
       "      <td>AMZN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-06 14:30:00+00:00</th>\n",
       "      <td>318.500000</td>\n",
       "      <td>322.250000</td>\n",
       "      <td>317.670105</td>\n",
       "      <td>321.929993</td>\n",
       "      <td>3490719</td>\n",
       "      <td>META</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-06 14:30:00+00:00</th>\n",
       "      <td>242.029999</td>\n",
       "      <td>246.199997</td>\n",
       "      <td>241.345093</td>\n",
       "      <td>242.919998</td>\n",
       "      <td>39241808</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Close        High         Low        Open  \\\n",
       "Datetime                                                                    \n",
       "2023-12-06 14:30:00+00:00  192.419998  194.759995  192.205002  194.449997   \n",
       "2023-12-06 14:30:00+00:00   46.444000   47.387001   46.361000   47.214500   \n",
       "2023-12-06 14:30:00+00:00  145.289993  147.850006  145.089996  147.580002   \n",
       "2023-12-06 14:30:00+00:00  318.500000  322.250000  317.670105  321.929993   \n",
       "2023-12-06 14:30:00+00:00  242.029999  246.199997  241.345093  242.919998   \n",
       "\n",
       "                             Volume ticker  \n",
       "Datetime                                    \n",
       "2023-12-06 14:30:00+00:00  11260893   AAPL  \n",
       "2023-12-06 14:30:00+00:00  11433656   NVDA  \n",
       "2023-12-06 14:30:00+00:00  10481937   AMZN  \n",
       "2023-12-06 14:30:00+00:00   3490719   META  \n",
       "2023-12-06 14:30:00+00:00  39241808   TSLA  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of tickers we have data for\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"NVDA\", \"META\", \"TSLA\"]\n",
    "\n",
    "price_dfs = []\n",
    "for ticker in tickers:\n",
    "    fname = f\"data/raw/market/{ticker}_1h.csv\"\n",
    "    # The CSV has 3 header lines, skip them and assign column names manually\n",
    "    df = pd.read_csv(fname, skiprows=3,\n",
    "                     names=[\"Datetime\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"])\n",
    "    # Convert to datetime and set UTC\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'], utc=True, errors='coerce')\n",
    "    df = df.dropna(subset=['Datetime'])  # drop any rows that didn't parse to datetime\n",
    "    df = df.set_index('Datetime')\n",
    "    df['ticker'] = ticker  # add ticker column\n",
    "    price_dfs.append(df)\n",
    "\n",
    "# Concatenate all tickers data\n",
    "prices_all = pd.concat(price_dfs)\n",
    "# Sort by datetime (and ticker, though ticker is not in index yet)\n",
    "prices_all = prices_all.sort_index()\n",
    "prices_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b5c2429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaant\\AppData\\Local\\Temp\\ipykernel_12468\\2596900630.py:24: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_index = pd.date_range(start=start, end=end, freq='H', tz='UTC')\n",
      "C:\\Users\\kaant\\AppData\\Local\\Temp\\ipykernel_12468\\2596900630.py:50: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  prices_all = gb.apply(_reindex_group)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>ticker</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-12-06 14:30:00+00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>192.419998</td>\n",
       "      <td>194.759995</td>\n",
       "      <td>192.205002</td>\n",
       "      <td>194.449997</td>\n",
       "      <td>11260893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-12-06 15:30:00+00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>193.095001</td>\n",
       "      <td>193.339996</td>\n",
       "      <td>192.360107</td>\n",
       "      <td>192.419998</td>\n",
       "      <td>4374474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-12-06 16:30:00+00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>192.830002</td>\n",
       "      <td>193.130005</td>\n",
       "      <td>192.470001</td>\n",
       "      <td>193.100006</td>\n",
       "      <td>3252326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-12-06 17:30:00+00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>192.905899</td>\n",
       "      <td>192.979996</td>\n",
       "      <td>192.369995</td>\n",
       "      <td>192.839996</td>\n",
       "      <td>3389634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-12-06 18:30:00+00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>192.779999</td>\n",
       "      <td>193.235001</td>\n",
       "      <td>192.740005</td>\n",
       "      <td>192.910004</td>\n",
       "      <td>2713794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime ticker       Close        High         Low  \\\n",
       "0 2023-12-06 14:30:00+00:00   AAPL  192.419998  194.759995  192.205002   \n",
       "1 2023-12-06 15:30:00+00:00   AAPL  193.095001  193.339996  192.360107   \n",
       "2 2023-12-06 16:30:00+00:00   AAPL  192.830002  193.130005  192.470001   \n",
       "3 2023-12-06 17:30:00+00:00   AAPL  192.905899  192.979996  192.369995   \n",
       "4 2023-12-06 18:30:00+00:00   AAPL  192.779999  193.235001  192.740005   \n",
       "\n",
       "         Open    Volume  \n",
       "0  194.449997  11260893  \n",
       "1  192.419998   4374474  \n",
       "2  193.100006   3252326  \n",
       "3  192.839996   3389634  \n",
       "4  192.910004   2713794  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Derive the full hourly date range from min to max timestamp in data\n",
    "# Robustly handle either a DatetimeIndex or a MultiIndex where one level is datetime\n",
    "def _infer_datetime_bounds(idx):\n",
    "    if isinstance(idx, pd.DatetimeIndex):\n",
    "        return idx.min(), idx.max()\n",
    "    if isinstance(idx, pd.MultiIndex):\n",
    "        # try to find a datetime level by dtype\n",
    "        for i in range(idx.nlevels):\n",
    "            lvl = idx.get_level_values(i)\n",
    "            if pd.api.types.is_datetime64_any_dtype(lvl):\n",
    "                return lvl.min(), lvl.max()\n",
    "        # try converting each level to datetime\n",
    "        for i in range(idx.nlevels):\n",
    "            lvl = pd.to_datetime(idx.get_level_values(i), utc=True, errors='coerce')\n",
    "            if not lvl.isna().all():\n",
    "                return lvl.min(), lvl.max()\n",
    "    # fallback: try converting the index itself\n",
    "    vals = pd.to_datetime(idx, utc=True, errors='coerce')\n",
    "    if isinstance(vals, (pd.DatetimeIndex, pd.Series)) and not vals.isna().all():\n",
    "        return vals.min(), vals.max()\n",
    "    raise TypeError('Could not determine datetime bounds from index')\n",
    "\n",
    "start, end = _infer_datetime_bounds(prices_all.index)\n",
    "full_index = pd.date_range(start=start, end=end, freq='H', tz='UTC')\n",
    "# Reindex each ticker sub-DataFrame to include all hours, forward-fill price data if needed\n",
    "# Choose grouping method dynamically to avoid ambiguity when 'ticker' is both an index level and a column\n",
    "if 'ticker' in prices_all.index.names:\n",
    "    gb = prices_all.groupby(level='ticker')\n",
    "else:\n",
    "    gb = prices_all.groupby('ticker')\n",
    "def _reindex_group(g):\n",
    "    if isinstance(g.index, pd.MultiIndex):\n",
    "        for i in range(g.index.nlevels):\n",
    "            vals = pd.to_datetime(g.index.get_level_values(i), utc=True, errors='coerce')\n",
    "            if not vals.isna().all():\n",
    "                g = g.copy()\n",
    "                g.index = vals\n",
    "                return g.reindex(full_index, method='ffill')\n",
    "        idx = pd.to_datetime([x[1] if isinstance(x, tuple) and len(x) > 1 else x for x in g.index], utc=True, errors='coerce')\n",
    "        g = g.copy()\n",
    "        g.index = idx\n",
    "        return g.reindex(full_index, method='ffill')\n",
    "    else:\n",
    "        idx = pd.to_datetime(g.index, utc=True, errors='coerce')\n",
    "        if idx.isna().all():\n",
    "            idx = pd.to_datetime([x[1] if isinstance(x, tuple) and len(x) > 1 else x for x in g.index], utc=True, errors='coerce')\n",
    "        g = g.copy()\n",
    "        g.index = idx\n",
    "        return g.reindex(full_index, method='ffill')\n",
    "prices_all = gb.apply(_reindex_group)\n",
    "# After groupby.apply the group key may become an index level named 'ticker',\n",
    "# but a 'ticker' column may already exist. Drop duplicate column before reset_index.\n",
    "prices_all = prices_all.drop(columns='ticker', errors='ignore')\n",
    "# The above creates a multi-index (ticker, datetime); bring the index levels back to columns\n",
    "prices_all = prices_all.reset_index(level=0).rename_axis('Datetime').reset_index()\n",
    "prices_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afdec956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI missing after fill: 0\n",
      "PPI unique values sample: [1.1056048120471784, 1.1056048120471784, 1.1056048120471784, 1.1056048120471784, 1.1056048120471784]\n",
      "                   Datetime ticker       Close   PPI_YoY\n",
      "0 2023-12-06 14:30:00+00:00   AAPL  192.419998  1.105605\n",
      "1 2023-12-06 15:30:00+00:00   AAPL  193.095001  1.105605\n",
      "2 2023-12-06 16:30:00+00:00   AAPL  192.830002  1.105605\n",
      "3 2023-12-06 17:30:00+00:00   AAPL  192.905899  1.105605\n",
      "4 2023-12-06 18:30:00+00:00   AAPL  192.779999  1.105605\n",
      "5 2023-12-06 19:30:00+00:00   AAPL  192.425995  1.105605\n",
      "6 2023-12-06 20:30:00+00:00   AAPL  192.309998  1.105605\n",
      "7 2023-12-06 21:30:00+00:00   AAPL  192.309998  1.105605\n",
      "8 2023-12-06 22:30:00+00:00   AAPL  192.309998  1.105605\n",
      "9 2023-12-06 23:30:00+00:00   AAPL  192.309998  1.105605\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load PPI hourly\n",
    "ppi = pd.read_csv(\"PPI_hourly.csv\")\n",
    "ppi[\"Datetime\"] = pd.to_datetime(ppi[\"Datetime\"], utc=True, errors=\"coerce\")\n",
    "ppi = ppi.dropna(subset=[\"Datetime\"]).sort_values(\"Datetime\")\n",
    "\n",
    "# Keep only what we need\n",
    "ppi = ppi[[\"Datetime\", \"PPI_YoY\"]]\n",
    "\n",
    "# 2) Merge into your existing prices_all\n",
    "# prices_all currently has columns: Datetime, ticker, Close, High, Low, Open, Volume\n",
    "prices_all[\"Datetime\"] = pd.to_datetime(prices_all[\"Datetime\"], utc=True, errors=\"coerce\")\n",
    "prices_all = prices_all.dropna(subset=[\"Datetime\"]).sort_values([\"ticker\", \"Datetime\"])\n",
    "\n",
    "prices_all = prices_all.merge(ppi, on=\"Datetime\", how=\"left\")\n",
    "\n",
    "# 3) Forward-fill PPI across time (global series), then (optional) backfill at the very start\n",
    "prices_all[\"PPI_YoY\"] = prices_all[\"PPI_YoY\"].ffill().bfill()\n",
    "\n",
    "# 4) Sanity checks (do these after every merge)\n",
    "print(\"PPI missing after fill:\", prices_all[\"PPI_YoY\"].isna().sum())\n",
    "print(\"PPI unique values sample:\", prices_all[\"PPI_YoY\"].dropna().head(5).tolist())\n",
    "print(prices_all[[\"Datetime\",\"ticker\",\"Close\",\"PPI_YoY\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acac5199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPI YoY missing: 0\n",
      "CPI MoM missing: 0\n",
      "                   Datetime ticker       Close   PPI_YoY   CPI_YoY   CPI_MoM\n",
      "0 2023-12-06 14:30:00+00:00   AAPL  192.419998  1.105605  3.246538  0.244648\n",
      "1 2023-12-06 15:30:00+00:00   AAPL  193.095001  1.105605  3.246538  0.244648\n",
      "2 2023-12-06 16:30:00+00:00   AAPL  192.830002  1.105605  3.246538  0.244648\n",
      "3 2023-12-06 17:30:00+00:00   AAPL  192.905899  1.105605  3.246538  0.244648\n",
      "4 2023-12-06 18:30:00+00:00   AAPL  192.779999  1.105605  3.246538  0.244648\n",
      "5 2023-12-06 19:30:00+00:00   AAPL  192.425995  1.105605  3.246538  0.244648\n",
      "6 2023-12-06 20:30:00+00:00   AAPL  192.309998  1.105605  3.246538  0.244648\n",
      "7 2023-12-06 21:30:00+00:00   AAPL  192.309998  1.105605  3.246538  0.244648\n",
      "8 2023-12-06 22:30:00+00:00   AAPL  192.309998  1.105605  3.246538  0.244648\n",
      "9 2023-12-06 23:30:00+00:00   AAPL  192.309998  1.105605  3.246538  0.244648\n"
     ]
    }
   ],
   "source": [
    "# 1) Load CPI hourly\n",
    "cpi = pd.read_csv(\"CPI_hourly.csv\")\n",
    "cpi[\"Datetime\"] = pd.to_datetime(cpi[\"Datetime\"], utc=True, errors=\"coerce\")\n",
    "cpi = cpi.dropna(subset=[\"Datetime\"]).sort_values(\"Datetime\")\n",
    "\n",
    "# Keep only needed columns\n",
    "cpi = cpi[[\"Datetime\", \"CPI_YoY\", \"CPI_MoM\"]]\n",
    "\n",
    "# 2) Merge into prices_all\n",
    "prices_all = prices_all.merge(cpi, on=\"Datetime\", how=\"left\")\n",
    "\n",
    "# 3) Forward-fill CPI (global macro)\n",
    "prices_all[[\"CPI_YoY\", \"CPI_MoM\"]] = (\n",
    "    prices_all[[\"CPI_YoY\", \"CPI_MoM\"]]\n",
    "    .ffill()\n",
    "    .bfill()\n",
    ")\n",
    "\n",
    "# 4) Sanity checks\n",
    "print(\"CPI YoY missing:\", prices_all[\"CPI_YoY\"].isna().sum())\n",
    "print(\"CPI MoM missing:\", prices_all[\"CPI_MoM\"].isna().sum())\n",
    "\n",
    "print(\n",
    "    prices_all[\n",
    "        [\"Datetime\", \"ticker\", \"Close\", \"PPI_YoY\", \"CPI_YoY\", \"CPI_MoM\"]\n",
    "    ].head(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f9e41f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Datetime', 'Fed_Funds_Rate'], dtype='object')\n",
      "Fed Funds missing: 0\n",
      "                   Datetime ticker       Close   CPI_YoY   PPI_YoY  \\\n",
      "0 2023-12-06 14:30:00+00:00   AAPL  192.419998  3.246538  1.105605   \n",
      "1 2023-12-06 15:30:00+00:00   AAPL  193.095001  3.246538  1.105605   \n",
      "2 2023-12-06 16:30:00+00:00   AAPL  192.830002  3.246538  1.105605   \n",
      "3 2023-12-06 17:30:00+00:00   AAPL  192.905899  3.246538  1.105605   \n",
      "4 2023-12-06 18:30:00+00:00   AAPL  192.779999  3.246538  1.105605   \n",
      "5 2023-12-06 19:30:00+00:00   AAPL  192.425995  3.246538  1.105605   \n",
      "6 2023-12-06 20:30:00+00:00   AAPL  192.309998  3.246538  1.105605   \n",
      "7 2023-12-06 21:30:00+00:00   AAPL  192.309998  3.246538  1.105605   \n",
      "8 2023-12-06 22:30:00+00:00   AAPL  192.309998  3.246538  1.105605   \n",
      "9 2023-12-06 23:30:00+00:00   AAPL  192.309998  3.246538  1.105605   \n",
      "\n",
      "   Fed_Funds_Rate  \n",
      "0             5.5  \n",
      "1             5.5  \n",
      "2             5.5  \n",
      "3             5.5  \n",
      "4             5.5  \n",
      "5             5.5  \n",
      "6             5.5  \n",
      "7             5.5  \n",
      "8             5.5  \n",
      "9             5.5  \n"
     ]
    }
   ],
   "source": [
    "# 1) Load FOMC-aligned hourly rate (THIS is the correct file)\n",
    "fomc = pd.read_csv(\"FOMC_rate_hourly.csv\")\n",
    "fomc[\"Datetime\"] = pd.to_datetime(fomc[\"Datetime\"], utc=True, errors=\"coerce\")\n",
    "fomc = fomc.dropna(subset=[\"Datetime\"]).sort_values(\"Datetime\")\n",
    "\n",
    "# Inspect columns\n",
    "print(fomc.columns)\n",
    "# Expect: [\"Datetime\", \"Fed_Funds_Rate\"]\n",
    "\n",
    "# 2) Merge into prices_all\n",
    "prices_all = prices_all.merge(\n",
    "    fomc[[\"Datetime\", \"Fed_Funds_Rate\"]],\n",
    "    on=\"Datetime\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 3) Forward-fill & backfill (policy regime)\n",
    "prices_all[\"Fed_Funds_Rate\"] = (\n",
    "    prices_all[\"Fed_Funds_Rate\"]\n",
    "    .ffill()\n",
    "    .bfill()\n",
    ")\n",
    "\n",
    "# 4) Sanity checks\n",
    "print(\"Fed Funds missing:\", prices_all[\"Fed_Funds_Rate\"].isna().sum())\n",
    "print(\n",
    "    prices_all[\n",
    "        [\"Datetime\", \"ticker\", \"Close\", \"CPI_YoY\", \"PPI_YoY\", \"Fed_Funds_Rate\"]\n",
    "    ].head(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfa54068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFP columns: ['Datetime', 'NonFarm_Payrolls_Change']\n",
      "Using NFP column: NonFarm_Payrolls_Change\n",
      "NFP missing: 0\n",
      "                   Datetime ticker       Close  NonFarm_Payrolls_Change\n",
      "0 2023-12-06 14:30:00+00:00   AAPL  192.419998                    141.0\n",
      "1 2023-12-06 15:30:00+00:00   AAPL  193.095001                    141.0\n",
      "2 2023-12-06 16:30:00+00:00   AAPL  192.830002                    141.0\n",
      "3 2023-12-06 17:30:00+00:00   AAPL  192.905899                    141.0\n",
      "4 2023-12-06 18:30:00+00:00   AAPL  192.779999                    141.0\n",
      "5 2023-12-06 19:30:00+00:00   AAPL  192.425995                    141.0\n",
      "6 2023-12-06 20:30:00+00:00   AAPL  192.309998                    141.0\n",
      "7 2023-12-06 21:30:00+00:00   AAPL  192.309998                    141.0\n",
      "8 2023-12-06 22:30:00+00:00   AAPL  192.309998                    141.0\n",
      "9 2023-12-06 23:30:00+00:00   AAPL  192.309998                    141.0\n"
     ]
    }
   ],
   "source": [
    "# 1) Load NFP hourly\n",
    "nfp = pd.read_csv(\"NFP_hourly.csv\")\n",
    "nfp[\"Datetime\"] = pd.to_datetime(nfp[\"Datetime\"], utc=True, errors=\"coerce\")\n",
    "nfp = nfp.dropna(subset=[\"Datetime\"]).sort_values(\"Datetime\")\n",
    "\n",
    "# Inspect columns\n",
    "print(\"NFP columns:\", nfp.columns.tolist())\n",
    "\n",
    "# Attempt to find the Non-Farm Payrolls change column automatically\n",
    "candidate = None\n",
    "patterns = [\"non\", \"nfp\", \"payroll\"]\n",
    "for col in nfp.columns:\n",
    "    if col.lower() == \"datetime\":\n",
    "        continue\n",
    "    low = col.lower()\n",
    "    if (\"non\" in low and \"farm\" in low) or \"payroll\" in low or \"nfp\" in low or \"nonfarm\" in low:\n",
    "        candidate = col\n",
    "        break\n",
    "# If none matched, pick first numeric column (excluding Datetime)\n",
    "if candidate is None:\n",
    "    for col in nfp.columns:\n",
    "        if col.lower() == \"datetime\":\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(nfp[col]):\n",
    "            candidate = col\n",
    "            break\n",
    "# Fallback to any non-datetime column\n",
    "if candidate is None:\n",
    "    cols = [c for c in nfp.columns if c.lower() != \"datetime\"]\n",
    "    if cols:\n",
    "        candidate = cols[0]\n",
    "if candidate is None:\n",
    "    raise KeyError(\"Could not find a suitable NonFarm_Payrolls_Change column in NFP file\")\n",
    "\n",
    "print(\"Using NFP column:\", candidate)\n",
    "nfp = nfp[[\"Datetime\", candidate]].rename(columns={candidate: \"NonFarm_Payrolls_Change\"})\n",
    "\n",
    "# 3) Merge into prices_all\n",
    "prices_all = prices_all.merge(nfp, on=\"Datetime\", how=\"left\")\n",
    "\n",
    "# 4) Forward-fill & backfill (labor regime)\n",
    "prices_all[\"NonFarm_Payrolls_Change\"] = (\n",
    "    prices_all[\"NonFarm_Payrolls_Change\"]\n",
    "    .ffill()\n",
    "    .bfill()\n",
    ")\n",
    "\n",
    "# 5) Sanity checks\n",
    "print(\"NFP missing:\", prices_all[\"NonFarm_Payrolls_Change\"].isna().sum())\n",
    "print(prices_all[[\"Datetime\",\"ticker\",\"Close\",\"NonFarm_Payrolls_Change\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ecec0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDP missing: 0\n",
      "                   Datetime ticker       Close  GDP_Growth_QoQ\n",
      "0 2023-12-06 14:30:00+00:00   AAPL  192.419998             4.9\n",
      "1 2023-12-06 15:30:00+00:00   AAPL  193.095001             4.9\n",
      "2 2023-12-06 16:30:00+00:00   AAPL  192.830002             4.9\n",
      "3 2023-12-06 17:30:00+00:00   AAPL  192.905899             4.9\n",
      "4 2023-12-06 18:30:00+00:00   AAPL  192.779999             4.9\n",
      "5 2023-12-06 19:30:00+00:00   AAPL  192.425995             4.9\n",
      "6 2023-12-06 20:30:00+00:00   AAPL  192.309998             4.9\n",
      "7 2023-12-06 21:30:00+00:00   AAPL  192.309998             4.9\n",
      "8 2023-12-06 22:30:00+00:00   AAPL  192.309998             4.9\n",
      "9 2023-12-06 23:30:00+00:00   AAPL  192.309998             4.9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gdp_hourly = pd.read_csv(\"GDP_hourly.csv\")\n",
    "gdp_hourly[\"Datetime\"] = pd.to_datetime(gdp_hourly[\"Datetime\"], utc=True, errors=\"coerce\")\n",
    "gdp_hourly = gdp_hourly.dropna(subset=[\"Datetime\"]).sort_values(\"Datetime\")\n",
    "\n",
    "# Merge: GDP is global, so merge on Datetime only\n",
    "prices_all[\"Datetime\"] = pd.to_datetime(prices_all[\"Datetime\"], utc=True, errors=\"coerce\")\n",
    "prices_all = prices_all.dropna(subset=[\"Datetime\"]).sort_values([\"ticker\", \"Datetime\"])\n",
    "\n",
    "prices_all = prices_all.merge(\n",
    "    gdp_hourly[[\"Datetime\", \"GDP_Growth_QoQ\"]],\n",
    "    on=\"Datetime\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Safety fill (should already be filled, but keep this pattern consistent)\n",
    "prices_all[\"GDP_Growth_QoQ\"] = prices_all[\"GDP_Growth_QoQ\"].ffill().bfill()\n",
    "\n",
    "# Sanity checks\n",
    "print(\"GDP missing:\", prices_all[\"GDP_Growth_QoQ\"].isna().sum())\n",
    "print(prices_all[[\"Datetime\", \"ticker\", \"Close\", \"GDP_Growth_QoQ\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "696a5dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Datetime', 'ticker', 'news_count', 'overall_sentiment_mean', 'ticker_sentiment_mean', 'ticker_relevance_mean']\n",
      "                    Datetime ticker  news_count  overall_sentiment_mean  \\\n",
      "0  2023-01-01 00:00:00+00:00   AAPL           0                     NaN   \n",
      "1  2023-01-01 01:00:00+00:00   AAPL           0                     NaN   \n",
      "2  2023-01-01 02:00:00+00:00   AAPL           0                     NaN   \n",
      "3  2023-01-01 03:00:00+00:00   AAPL           0                     NaN   \n",
      "4  2023-01-01 04:00:00+00:00   AAPL           0                     NaN   \n",
      "\n",
      "   ticker_sentiment_mean  ticker_relevance_mean  \n",
      "0                    NaN                    NaN  \n",
      "1                    NaN                    NaN  \n",
      "2                    NaN                    NaN  \n",
      "3                    NaN                    NaN  \n",
      "4                    NaN                    NaN  \n",
      "                         Datetime ticker  news_count  overall_sentiment_mean  \\\n",
      "180602  2025-12-10 20:00:00+00:00   TSLA           1                0.306014   \n",
      "180603  2025-12-10 21:00:00+00:00   TSLA           0                     NaN   \n",
      "180604  2025-12-10 22:00:00+00:00   TSLA           1                0.095975   \n",
      "180605  2025-12-10 23:00:00+00:00   TSLA           1                0.193779   \n",
      "180606  2025-12-11 00:00:00+00:00   TSLA           0                     NaN   \n",
      "\n",
      "        ticker_sentiment_mean  ticker_relevance_mean  \n",
      "180602               0.325369               0.896651  \n",
      "180603                    NaN                    NaN  \n",
      "180604               0.066908               0.633076  \n",
      "180605               0.113061               0.626153  \n",
      "180606                    NaN                    NaN  \n",
      "Sentiment NaT count: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Try opening your produced sentiment file (adjust filename as needed)\n",
    "sent = pd.read_csv(\"news_sentiment_hourly.csv\")  # rename if yours differs\n",
    "print(sent.columns.tolist())\n",
    "print(sent.head(5))\n",
    "print(sent.tail(5))\n",
    "\n",
    "# Confirm timestamp parsing\n",
    "sent[\"Datetime\"] = pd.to_datetime(sent[\"Datetime\"], utc=True, errors=\"coerce\")\n",
    "print(\"Sentiment NaT count:\", sent[\"Datetime\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f592dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News tickers: ['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA']\n",
      "Market tickers: ['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA']\n"
     ]
    }
   ],
   "source": [
    "print(\"News tickers:\", sorted(sent[\"ticker\"].unique().tolist()))\n",
    "print(\"Market tickers:\", sorted(prices_all[\"ticker\"].unique().tolist()))\n",
    "\n",
    "sent = sent.sort_values([\"ticker\", \"Datetime\"])\n",
    "prices_all = prices_all.sort_values([\"ticker\", \"Datetime\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51a65d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime\n",
      "30    180607\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Align news timestamps to market bar timestamps (HH:30)\n",
    "sent = sent.copy()\n",
    "sent[\"Datetime\"] = pd.to_datetime(sent[\"Datetime\"], utc=True, errors=\"coerce\")\n",
    "sent = sent.dropna(subset=[\"Datetime\"])\n",
    "\n",
    "# shift from :00 to :30 so it matches your OHLCV timestamps\n",
    "sent[\"Datetime\"] = sent[\"Datetime\"] + pd.Timedelta(minutes=30)\n",
    "\n",
    "# sanity check: should now show :30 minutes\n",
    "print(sent[\"Datetime\"].dt.minute.value_counts().head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979dda1f",
   "metadata": {},
   "source": [
    "### Don't run unless you are sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07f0a1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner columns: ['Datetime', 'ticker', 'Close', 'High', 'Low', 'Open', 'Volume', 'PPI_YoY', 'CPI_YoY', 'CPI_MoM', 'Fed_Funds_Rate', 'NonFarm_Payrolls_Change', 'GDP_Growth_QoQ', 'news_count']\n",
      "inner-merged rows: 122689\n",
      "using news_count -> positive news rows: 7791\n"
     ]
    }
   ],
   "source": [
    "# Do inner merge but avoid column-name collisions by specifying suffixes\n",
    "inner = prices_all.merge(\n",
    "    sent[[\"Datetime\",\"ticker\",\"news_count\"]],\n",
    "    on=[\"Datetime\",\"ticker\"],\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_price\", \"_sent\")\n",
    ")\n",
    "print('inner columns:', list(inner.columns))\n",
    "print('inner-merged rows:', len(inner))\n",
    "# Prefer the sent-side news_count if it exists after the merge\n",
    "news_col = None\n",
    "for candidate in ['news_count_sent', 'news_count_y', 'news_count']:\n",
    "    if candidate in inner.columns:\n",
    "        news_col = candidate\n",
    "        break\n",
    "if news_col is None:\n",
    "    print('No news_count column found in inner; available cols:', inner.columns.tolist())\n",
    "else:\n",
    "    print('using', news_col, '-> positive news rows:', int((inner[news_col] > 0).sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999af8b9",
   "metadata": {},
   "source": [
    "### Don't run unless you are sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "539e2f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment columns now present in prices_all: []\n"
     ]
    }
   ],
   "source": [
    "# Remove previously-merged sentiment columns to avoid duplicates\n",
    "to_drop = [\n",
    "    \"news_count\", \"has_news\",\n",
    "    \"overall_sentiment_ffill\",\n",
    "    \"ticker_sentiment_ffill\",\n",
    "    \"ticker_relevance_ffill\",\n",
    "    # if prior merges created suffixed columns, drop them too:\n",
    "    \"news_count_x\", \"news_count_y\",\n",
    "    \"has_news_x\", \"has_news_y\",\n",
    "    \"overall_sentiment_ffill_x\", \"overall_sentiment_ffill_y\",\n",
    "    \"ticker_sentiment_ffill_x\", \"ticker_sentiment_ffill_y\",\n",
    "    \"ticker_relevance_ffill_x\", \"ticker_relevance_ffill_y\",\n",
    "]\n",
    "prices_all = prices_all.drop(columns=to_drop, errors=\"ignore\")\n",
    "\n",
    "print(\"Sentiment columns now present in prices_all:\",\n",
    "      [c for c in prices_all.columns if \"sentiment\" in c.lower() or \"news_count\" in c.lower() or \"has_news\" in c.lower()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87e7568d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner-merged rows: 0\n",
      "inner rows with probe>0: 0\n",
      "Empty DataFrame\n",
      "Columns: [Datetime, ticker, probe]\n",
      "Index: []\n",
      "\n",
      "Preview:\n",
      "                    Datetime ticker       Close  news_count  has_news  \\\n",
      "0  2023-12-06 14:30:00+00:00   AAPL  192.419998         0.0       0.0   \n",
      "1  2023-12-06 15:30:00+00:00   AAPL  193.095001         0.0       0.0   \n",
      "2  2023-12-06 16:30:00+00:00   AAPL  192.830002         0.0       0.0   \n",
      "3  2023-12-06 17:30:00+00:00   AAPL  192.905899         0.0       0.0   \n",
      "4  2023-12-06 18:30:00+00:00   AAPL  192.779999         0.0       0.0   \n",
      "5  2023-12-06 19:30:00+00:00   AAPL  192.425995         0.0       0.0   \n",
      "6  2023-12-06 20:30:00+00:00   AAPL  192.309998         0.0       0.0   \n",
      "7  2023-12-06 21:30:00+00:00   AAPL  192.309998         0.0       0.0   \n",
      "8  2023-12-06 22:30:00+00:00   AAPL  192.309998         0.0       0.0   \n",
      "9  2023-12-06 23:30:00+00:00   AAPL  192.309998         0.0       0.0   \n",
      "10 2023-12-07 00:30:00+00:00   AAPL  192.309998         0.0       0.0   \n",
      "11 2023-12-07 01:30:00+00:00   AAPL  192.309998         0.0       0.0   \n",
      "12 2023-12-07 02:30:00+00:00   AAPL  192.309998         0.0       0.0   \n",
      "13 2023-12-07 03:30:00+00:00   AAPL  192.309998         0.0       0.0   \n",
      "14 2023-12-07 04:30:00+00:00   AAPL  192.309998         0.0       0.0   \n",
      "\n",
      "    ticker_sentiment_ffill  \n",
      "0                      0.0  \n",
      "1                      0.0  \n",
      "2                      0.0  \n",
      "3                      0.0  \n",
      "4                      0.0  \n",
      "5                      0.0  \n",
      "6                      0.0  \n",
      "7                      0.0  \n",
      "8                      0.0  \n",
      "9                      0.0  \n",
      "10                     0.0  \n",
      "11                     0.0  \n",
      "12                     0.0  \n",
      "13                     0.0  \n",
      "14                     0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# 0) Clean copies\n",
    "# ============================\n",
    "prices_all = prices_all.copy()\n",
    "sent = sent.copy()\n",
    "\n",
    "prices_all[\"Datetime\"] = pd.to_datetime(prices_all[\"Datetime\"], utc=True, errors=\"coerce\")\n",
    "sent[\"Datetime\"] = pd.to_datetime(sent[\"Datetime\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "prices_all = prices_all.dropna(subset=[\"Datetime\"]).sort_values([\"ticker\",\"Datetime\"])\n",
    "sent = sent.dropna(subset=[\"Datetime\"]).sort_values([\"ticker\",\"Datetime\"])\n",
    "\n",
    "# ============================\n",
    "# 1) ALIGNMENT FIX (FINAL)\n",
    "# ============================\n",
    "# Sentiment is hourly at HH:00, market bars close at HH:30\n",
    "sent[\"Datetime\"] = sent[\"Datetime\"] + pd.Timedelta(minutes=30)\n",
    "\n",
    "# ============================\n",
    "# 2) Option A features\n",
    "# ============================\n",
    "sent[\"has_news\"] = (sent[\"news_count\"] > 0).astype(int)\n",
    "\n",
    "sent[\"overall_sentiment_ffill\"] = sent.groupby(\"ticker\")[\"overall_sentiment_mean\"].ffill()\n",
    "sent[\"ticker_sentiment_ffill\"]  = sent.groupby(\"ticker\")[\"ticker_sentiment_mean\"].ffill()\n",
    "sent[\"ticker_relevance_ffill\"]  = sent.groupby(\"ticker\")[\"ticker_relevance_mean\"].ffill()\n",
    "\n",
    "sent[[\n",
    "    \"overall_sentiment_ffill\",\n",
    "    \"ticker_sentiment_ffill\",\n",
    "    \"ticker_relevance_ffill\"\n",
    "]] = sent[[\n",
    "    \"overall_sentiment_ffill\",\n",
    "    \"ticker_sentiment_ffill\",\n",
    "    \"ticker_relevance_ffill\"\n",
    "]].fillna(0.0)\n",
    "\n",
    "# ============================\n",
    "# 3) Drop old sentiment cols (safety)\n",
    "# ============================\n",
    "sentiment_cols = [\n",
    "    \"news_count\",\"has_news\",\n",
    "    \"overall_sentiment_ffill\",\n",
    "    \"ticker_sentiment_ffill\",\n",
    "    \"ticker_relevance_ffill\"\n",
    "]\n",
    "prices_all = prices_all.drop(columns=sentiment_cols, errors=\"ignore\")\n",
    "\n",
    "# ============================\n",
    "# 4) Merge (this WILL work now)\n",
    "# ============================\n",
    "sent_merge = sent[[\n",
    "    \"Datetime\",\"ticker\",\n",
    "    \"news_count\",\"has_news\",\n",
    "    \"overall_sentiment_ffill\",\n",
    "    \"ticker_sentiment_ffill\",\n",
    "    \"ticker_relevance_ffill\"\n",
    "]]\n",
    "\n",
    "prices_all = prices_all.merge(\n",
    "    sent_merge,\n",
    "    on=[\"Datetime\",\"ticker\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 5) Fill gaps safely\n",
    "# ============================\n",
    "prices_all[\"news_count\"] = prices_all[\"news_count\"].fillna(0)\n",
    "prices_all[\"has_news\"] = prices_all[\"has_news\"].fillna(0)\n",
    "prices_all[\"overall_sentiment_ffill\"] = prices_all[\"overall_sentiment_ffill\"].fillna(0.0)\n",
    "prices_all[\"ticker_sentiment_ffill\"] = prices_all[\"ticker_sentiment_ffill\"].fillna(0.0)\n",
    "prices_all[\"ticker_relevance_ffill\"] = prices_all[\"ticker_relevance_ffill\"].fillna(0.0)\n",
    "\n",
    "# ============================\n",
    "# 6) HARD PROOF\n",
    "# ============================\n",
    "inner = prices_all.merge(\n",
    "    sent[[\"Datetime\",\"ticker\",\"news_count\"]].rename(columns={\"news_count\":\"probe\"}),\n",
    "    on=[\"Datetime\",\"ticker\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"inner-merged rows:\", len(inner))\n",
    "print(\"inner rows with probe>0:\", int((inner[\"probe\"] > 0).sum()))\n",
    "print(inner.loc[inner[\"probe\"] > 0, [\"Datetime\",\"ticker\",\"probe\"]].head(10))\n",
    "\n",
    "print(\"\\nPreview:\")\n",
    "print(prices_all[\n",
    "    [\"Datetime\",\"ticker\",\"Close\",\"news_count\",\"has_news\",\"ticker_sentiment_ffill\"]\n",
    "].head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "499fa90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 28 rows without 4h-ahead target.\n",
      "\n",
      "Target summary statistics:\n",
      "count    122661.000000\n",
      "mean          0.000154\n",
      "std           0.008425\n",
      "min          -0.190422\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           0.199039\n",
      "Name: target_log_return_4h, dtype: float64\n",
      "\n",
      "Sample rows:\n",
      "                   Datetime ticker       Close  Close_t_plus_4  \\\n",
      "0 2023-12-06 14:30:00+00:00   AAPL  192.419998      192.779999   \n",
      "1 2023-12-06 15:30:00+00:00   AAPL  193.095001      192.425995   \n",
      "2 2023-12-06 16:30:00+00:00   AAPL  192.830002      192.309998   \n",
      "3 2023-12-06 17:30:00+00:00   AAPL  192.905899      192.309998   \n",
      "4 2023-12-06 18:30:00+00:00   AAPL  192.779999      192.309998   \n",
      "5 2023-12-06 19:30:00+00:00   AAPL  192.425995      192.309998   \n",
      "6 2023-12-06 20:30:00+00:00   AAPL  192.309998      192.309998   \n",
      "7 2023-12-06 21:30:00+00:00   AAPL  192.309998      192.309998   \n",
      "8 2023-12-06 22:30:00+00:00   AAPL  192.309998      192.309998   \n",
      "9 2023-12-06 23:30:00+00:00   AAPL  192.309998      192.309998   \n",
      "\n",
      "   target_log_return_4h  \n",
      "0              0.001869  \n",
      "1             -0.003471  \n",
      "2             -0.002700  \n",
      "3             -0.003094  \n",
      "4             -0.002441  \n",
      "5             -0.000603  \n",
      "6              0.000000  \n",
      "7              0.000000  \n",
      "8              0.000000  \n",
      "9              0.000000  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ensure proper sorting\n",
    "prices_all = prices_all.sort_values([\"ticker\", \"Datetime\"]).reset_index(drop=True)\n",
    "\n",
    "HORIZON = 4  # 4-hour ahead target\n",
    "\n",
    "# Compute future close per ticker\n",
    "prices_all[\"Close_t_plus_4\"] = (\n",
    "    prices_all\n",
    "    .groupby(\"ticker\")[\"Close\"]\n",
    "    .shift(-HORIZON)\n",
    ")\n",
    "\n",
    "# Compute next-4-hour log return\n",
    "prices_all[\"target_log_return_4h\"] = np.log(\n",
    "    prices_all[\"Close_t_plus_4\"] / prices_all[\"Close\"]\n",
    ")\n",
    "\n",
    "# Drop rows without a valid target (last 4 rows per ticker)\n",
    "before = len(prices_all)\n",
    "prices_all = prices_all.dropna(subset=[\"target_log_return_4h\"]).reset_index(drop=True)\n",
    "after = len(prices_all)\n",
    "\n",
    "print(f\"Dropped {before - after} rows without 4h-ahead target.\")\n",
    "\n",
    "# Sanity checks\n",
    "print(\"\\nTarget summary statistics:\")\n",
    "print(prices_all[\"target_log_return_4h\"].describe())\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "print(\n",
    "    prices_all[\n",
    "        [\"Datetime\",\"ticker\",\"Close\",\"Close_t_plus_4\",\"target_log_return_4h\"]\n",
    "    ].head(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "643e9cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 7 rows due to 1h return NaN.\n",
      "\n",
      "log_return_1h summary:\n",
      "count    122654.000000\n",
      "mean          0.000039\n",
      "std           0.004219\n",
      "min          -0.139523\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           0.182174\n",
      "Name: log_return_1h, dtype: float64\n",
      "\n",
      "Sample rows:\n",
      "                   Datetime ticker       Close  log_return_1h  \\\n",
      "0 2023-12-06 15:30:00+00:00   AAPL  193.095001       0.003502   \n",
      "1 2023-12-06 16:30:00+00:00   AAPL  192.830002      -0.001373   \n",
      "2 2023-12-06 17:30:00+00:00   AAPL  192.905899       0.000394   \n",
      "3 2023-12-06 18:30:00+00:00   AAPL  192.779999      -0.000653   \n",
      "4 2023-12-06 19:30:00+00:00   AAPL  192.425995      -0.001838   \n",
      "5 2023-12-06 20:30:00+00:00   AAPL  192.309998      -0.000603   \n",
      "6 2023-12-06 21:30:00+00:00   AAPL  192.309998       0.000000   \n",
      "7 2023-12-06 22:30:00+00:00   AAPL  192.309998       0.000000   \n",
      "8 2023-12-06 23:30:00+00:00   AAPL  192.309998       0.000000   \n",
      "9 2023-12-07 00:30:00+00:00   AAPL  192.309998       0.000000   \n",
      "\n",
      "   target_log_return_4h  \n",
      "0             -0.003471  \n",
      "1             -0.002700  \n",
      "2             -0.003094  \n",
      "3             -0.002441  \n",
      "4             -0.000603  \n",
      "5              0.000000  \n",
      "6              0.000000  \n",
      "7              0.000000  \n",
      "8              0.000000  \n",
      "9              0.000000  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ensure proper ordering\n",
    "prices_all = prices_all.sort_values([\"ticker\", \"Datetime\"]).reset_index(drop=True)\n",
    "\n",
    "# 1-hour log return per ticker (index-safe)\n",
    "prices_all[\"log_return_1h\"] = prices_all.groupby(\"ticker\")[\"Close\"].transform(\n",
    "    lambda x: np.log(x / x.shift(1))\n",
    ")\n",
    "\n",
    "# Drop the first row per ticker (no previous hour)\n",
    "before = len(prices_all)\n",
    "prices_all = prices_all.dropna(subset=[\"log_return_1h\"]).reset_index(drop=True)\n",
    "after = len(prices_all)\n",
    "\n",
    "print(f\"Dropped {before - after} rows due to 1h return NaN.\")\n",
    "\n",
    "# Sanity checks\n",
    "print(\"\\nlog_return_1h summary:\")\n",
    "print(prices_all[\"log_return_1h\"].describe())\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "print(\n",
    "    prices_all[\n",
    "        [\"Datetime\",\"ticker\",\"Close\",\"log_return_1h\",\"target_log_return_4h\"]\n",
    "    ].head(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "771d7ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 161 rows due to rolling volatility warm-up.\n",
      "\n",
      "vol_12h summary:\n",
      "count    122493.000000\n",
      "mean          0.002144\n",
      "std           0.003635\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000325\n",
      "75%           0.003054\n",
      "max           0.052930\n",
      "Name: vol_12h, dtype: float64\n",
      "\n",
      "vol_24h summary:\n",
      "count    122493.000000\n",
      "mean          0.002738\n",
      "std           0.003213\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.002025\n",
      "75%           0.003834\n",
      "max           0.037465\n",
      "Name: vol_24h, dtype: float64\n",
      "\n",
      "Sample rows:\n",
      "                   Datetime ticker  log_return_1h   vol_12h   vol_24h  \\\n",
      "0 2023-12-07 14:30:00+00:00   AAPL       0.012480  0.003603  0.002705   \n",
      "1 2023-12-07 15:30:00+00:00   AAPL      -0.002185  0.003713  0.002679   \n",
      "2 2023-12-07 16:30:00+00:00   AAPL       0.003288  0.003765  0.002724   \n",
      "3 2023-12-07 17:30:00+00:00   AAPL      -0.002620  0.003910  0.002796   \n",
      "4 2023-12-07 18:30:00+00:00   AAPL       0.000206  0.003906  0.002788   \n",
      "5 2023-12-07 19:30:00+00:00   AAPL      -0.001544  0.003965  0.002779   \n",
      "6 2023-12-07 20:30:00+00:00   AAPL       0.000360  0.003959  0.002771   \n",
      "7 2023-12-07 21:30:00+00:00   AAPL       0.000000  0.003959  0.002771   \n",
      "8 2023-12-07 22:30:00+00:00   AAPL       0.000000  0.003959  0.002771   \n",
      "9 2023-12-07 23:30:00+00:00   AAPL       0.000000  0.003959  0.002771   \n",
      "\n",
      "   target_log_return_4h  \n",
      "0             -0.001310  \n",
      "1             -0.000669  \n",
      "2             -0.003597  \n",
      "3             -0.000978  \n",
      "4             -0.001183  \n",
      "5              0.000360  \n",
      "6              0.000000  \n",
      "7              0.000000  \n",
      "8              0.000000  \n",
      "9              0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Ensure sorted\n",
    "prices_all = prices_all.sort_values([\"ticker\", \"Datetime\"]).reset_index(drop=True)\n",
    "\n",
    "# Rolling vol per ticker (std of 1h log returns)\n",
    "# min_periods makes early values available; weâ€™ll drop remaining NaNs after.\n",
    "prices_all[\"vol_12h\"] = prices_all.groupby(\"ticker\")[\"log_return_1h\"].transform(\n",
    "    lambda x: x.rolling(window=12, min_periods=12).std()\n",
    ")\n",
    "\n",
    "prices_all[\"vol_24h\"] = prices_all.groupby(\"ticker\")[\"log_return_1h\"].transform(\n",
    "    lambda x: x.rolling(window=24, min_periods=24).std()\n",
    ")\n",
    "\n",
    "# Drop rows where vol_24h is NaN (first 23 rows per ticker)\n",
    "before = len(prices_all)\n",
    "prices_all = prices_all.dropna(subset=[\"vol_24h\"]).reset_index(drop=True)\n",
    "after = len(prices_all)\n",
    "\n",
    "print(f\"Dropped {before - after} rows due to rolling volatility warm-up.\")\n",
    "\n",
    "# Sanity checks\n",
    "print(\"\\nvol_12h summary:\")\n",
    "print(prices_all[\"vol_12h\"].describe())\n",
    "\n",
    "print(\"\\nvol_24h summary:\")\n",
    "print(prices_all[\"vol_24h\"].describe())\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "print(\n",
    "    prices_all[\n",
    "        [\"Datetime\",\"ticker\",\"log_return_1h\",\"vol_12h\",\"vol_24h\",\"target_log_return_4h\"]\n",
    "    ].head(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e5a6f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 34853 rows due to volume z-score warm-up.\n",
      "\n",
      "log_volume summary:\n",
      "count    87640.000000\n",
      "mean        15.246394\n",
      "std          0.947637\n",
      "min          0.000000\n",
      "25%         14.641866\n",
      "50%         15.185104\n",
      "75%         15.754516\n",
      "max         19.311836\n",
      "Name: log_volume, dtype: float64\n",
      "\n",
      "vol_z_24h summary:\n",
      "count    87640.000000\n",
      "mean        -0.006338\n",
      "std          1.166965\n",
      "min         -4.694855\n",
      "25%         -0.509627\n",
      "50%         -0.076844\n",
      "75%          0.400778\n",
      "max          4.694855\n",
      "Name: vol_z_24h, dtype: float64\n",
      "\n",
      "Sample rows:\n",
      "                   Datetime ticker    Volume  log_volume  vol_z_24h  \\\n",
      "0 2023-12-08 13:30:00+00:00   AAPL   4479168   15.314948  -0.079984   \n",
      "1 2023-12-08 14:30:00+00:00   AAPL  11185810   16.230157   4.083575   \n",
      "2 2023-12-08 15:30:00+00:00   AAPL   5158425   15.456142   0.572241   \n",
      "3 2023-12-08 16:30:00+00:00   AAPL   5398492   15.501630   0.739879   \n",
      "4 2023-12-08 17:30:00+00:00   AAPL   4520094   15.324044  -0.059071   \n",
      "5 2023-12-08 18:30:00+00:00   AAPL   3904469   15.177633  -0.786099   \n",
      "6 2023-12-08 19:30:00+00:00   AAPL   5640599   15.545501   0.887519   \n",
      "7 2023-12-08 20:30:00+00:00   AAPL   5190666   15.462373   0.432421   \n",
      "8 2023-12-08 21:30:00+00:00   AAPL   5190666   15.462373   0.400695   \n",
      "9 2023-12-08 22:30:00+00:00   AAPL   5190666   15.462373   0.369431   \n",
      "\n",
      "   news_count  has_news  target_log_return_4h  \n",
      "0         0.0       0.0              0.005135  \n",
      "1         0.0       0.0              0.005457  \n",
      "2         0.0       0.0              0.005019  \n",
      "3         0.0       0.0              0.003173  \n",
      "4         0.0       0.0              0.002455  \n",
      "5         0.0       0.0              0.000026  \n",
      "6         0.0       0.0             -0.000102  \n",
      "7         0.0       0.0              0.000000  \n",
      "8         0.0       0.0              0.000000  \n",
      "9         0.0       0.0              0.000000  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ensure proper ordering\n",
    "prices_all = prices_all.sort_values([\"ticker\", \"Datetime\"]).reset_index(drop=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Log volume\n",
    "# ----------------------------\n",
    "# Add +1 to avoid log(0)\n",
    "prices_all[\"log_volume\"] = np.log(prices_all[\"Volume\"] + 1)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Rolling volume z-score (24h)\n",
    "# ----------------------------\n",
    "def rolling_zscore(x, window=24):\n",
    "    mu = x.rolling(window, min_periods=window).mean()\n",
    "    sd = x.rolling(window, min_periods=window).std()\n",
    "    return (x - mu) / sd\n",
    "\n",
    "prices_all[\"vol_z_24h\"] = prices_all.groupby(\"ticker\")[\"log_volume\"].transform(\n",
    "    lambda x: rolling_zscore(x, window=24)\n",
    ")\n",
    "\n",
    "# Drop warm-up NaNs\n",
    "before = len(prices_all)\n",
    "prices_all = prices_all.dropna(subset=[\"vol_z_24h\"]).reset_index(drop=True)\n",
    "after = len(prices_all)\n",
    "\n",
    "print(f\"Dropped {before - after} rows due to volume z-score warm-up.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Sanity checks\n",
    "# ----------------------------\n",
    "print(\"\\nlog_volume summary:\")\n",
    "print(prices_all[\"log_volume\"].describe())\n",
    "\n",
    "print(\"\\nvol_z_24h summary:\")\n",
    "print(prices_all[\"vol_z_24h\"].describe())\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "print(\n",
    "    prices_all[\n",
    "        [\n",
    "            \"Datetime\",\"ticker\",\"Volume\",\n",
    "            \"log_volume\",\"vol_z_24h\",\n",
    "            \"news_count\",\"has_news\",\n",
    "            \"target_log_return_4h\"\n",
    "        ]\n",
    "    ].head(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d10fcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 16\n",
      "['log_return_1h', 'vol_12h', 'vol_24h', 'log_volume', 'vol_z_24h', 'news_count', 'has_news', 'ticker_sentiment_ffill', 'overall_sentiment_ffill', 'ticker_relevance_ffill', 'CPI_YoY', 'CPI_MoM', 'PPI_YoY', 'GDP_Growth_QoQ', 'Fed_Funds_Rate', 'NonFarm_Payrolls_Change']\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Feature groups\n",
    "# ============================\n",
    "\n",
    "RETURN_COLS = [\n",
    "    \"log_return_1h\"\n",
    "]\n",
    "\n",
    "VOL_COLS = [\n",
    "    \"vol_12h\",\n",
    "    \"vol_24h\"\n",
    "]\n",
    "\n",
    "VOLUME_COLS = [\n",
    "    \"log_volume\",\n",
    "    \"vol_z_24h\"\n",
    "]\n",
    "\n",
    "SENTIMENT_COLS = [\n",
    "    \"news_count\",\n",
    "    \"has_news\",\n",
    "    \"ticker_sentiment_ffill\",\n",
    "    \"overall_sentiment_ffill\",\n",
    "    \"ticker_relevance_ffill\"\n",
    "]\n",
    "\n",
    "MACRO_COLS = [\n",
    "    \"CPI_YoY\", \"CPI_MoM\",\n",
    "    \"PPI_YoY\",\n",
    "    \"GDP_Growth_QoQ\",\n",
    "    \"Fed_Funds_Rate\",\n",
    "    \"NonFarm_Payrolls_Change\"\n",
    "]\n",
    "\n",
    "TARGET_COL = \"target_log_return_4h\"\n",
    "\n",
    "ALL_FEATURES = (\n",
    "    RETURN_COLS\n",
    "    + VOL_COLS\n",
    "    + VOLUME_COLS\n",
    "    + SENTIMENT_COLS\n",
    "    + MACRO_COLS\n",
    ")\n",
    "\n",
    "print(\"Total features:\", len(ALL_FEATURES))\n",
    "print(ALL_FEATURES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6fcbed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "train    61355\n",
      "val      13146\n",
      "test     13139\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Split boundaries:\n",
      "Train end: 2025-05-02 17:30:00+00:00\n",
      "Val end: 2025-08-20 14:30:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Temporal split\n",
    "# ============================\n",
    "\n",
    "prices_all = prices_all.sort_values([\"Datetime\",\"ticker\"]).reset_index(drop=True)\n",
    "\n",
    "unique_times = prices_all[\"Datetime\"].sort_values().unique()\n",
    "n = len(unique_times)\n",
    "\n",
    "train_end = unique_times[int(n * 0.70)]\n",
    "val_end   = unique_times[int(n * 0.85)]\n",
    "\n",
    "prices_all[\"split\"] = \"test\"\n",
    "prices_all.loc[prices_all[\"Datetime\"] <= train_end, \"split\"] = \"train\"\n",
    "prices_all.loc[\n",
    "    (prices_all[\"Datetime\"] > train_end) &\n",
    "    (prices_all[\"Datetime\"] <= val_end),\n",
    "    \"split\"\n",
    "] = \"val\"\n",
    "\n",
    "print(prices_all[\"split\"].value_counts())\n",
    "print(\"\\nSplit boundaries:\")\n",
    "print(\"Train end:\", train_end)\n",
    "print(\"Val end:\", val_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cb3f463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============================\n",
    "# Fit scalers on TRAIN only\n",
    "# ============================\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "def fit_scaler(cols):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(prices_all.loc[prices_all[\"split\"] == \"train\", cols])\n",
    "    return scaler\n",
    "\n",
    "scalers[\"vol\"]    = fit_scaler(VOL_COLS)\n",
    "scalers[\"volume\"] = fit_scaler(VOLUME_COLS)\n",
    "scalers[\"macro\"]  = fit_scaler(MACRO_COLS)\n",
    "\n",
    "# Apply transforms\n",
    "prices_all[VOL_COLS] = scalers[\"vol\"].transform(prices_all[VOL_COLS])\n",
    "prices_all[VOLUME_COLS] = scalers[\"volume\"].transform(prices_all[VOLUME_COLS])\n",
    "prices_all[MACRO_COLS] = scalers[\"macro\"].transform(prices_all[MACRO_COLS])\n",
    "\n",
    "print(\"Scaling complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce23a815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN stats (should be ~0 mean, ~1 std):\n",
      "       vol_12h       vol_24h    log_volume     vol_z_24h       CPI_YoY  \\\n",
      "mean  0.000000  8.894089e-17  8.745854e-16 -1.227570e-17  7.708210e-16   \n",
      "std   1.000008  1.000008e+00  1.000008e+00  1.000008e+00  1.000008e+00   \n",
      "\n",
      "           CPI_MoM       PPI_YoY  GDP_Growth_QoQ  Fed_Funds_Rate  \\\n",
      "mean -7.411741e-17  5.929393e-17    1.482348e-16    5.336453e-16   \n",
      "std   1.000008e+00  1.000008e+00    1.000008e+00    1.000008e+00   \n",
      "\n",
      "      NonFarm_Payrolls_Change  \n",
      "mean                 0.000000  \n",
      "std                  1.000008  \n",
      "\n",
      "VAL stats (should be shifted, NOT zero-mean):\n",
      "       vol_12h   vol_24h  log_volume  vol_z_24h   CPI_YoY   CPI_MoM   PPI_YoY  \\\n",
      "mean -0.152178 -0.235011    0.105932  -0.015499 -1.510318 -1.229532  0.304840   \n",
      "std   0.796431  0.792317    0.961873   1.013466  0.517528  0.580867  0.329469   \n",
      "\n",
      "      GDP_Growth_QoQ  Fed_Funds_Rate  NonFarm_Payrolls_Change  \n",
      "mean       -2.686173   -1.436314e+00                -1.179440  \n",
      "std         1.483466    3.865944e-13                 0.846847  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTRAIN stats (should be ~0 mean, ~1 std):\")\n",
    "print(prices_all.loc[prices_all[\"split\"]==\"train\", VOL_COLS + VOLUME_COLS + MACRO_COLS].describe().loc[[\"mean\",\"std\"]])\n",
    "\n",
    "print(\"\\nVAL stats (should be shifted, NOT zero-mean):\")\n",
    "print(prices_all.loc[prices_all[\"split\"]==\"val\", VOL_COLS + VOLUME_COLS + MACRO_COLS].describe().loc[[\"mean\",\"std\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ada6c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Feature count: 16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Choose which features go into X (exclude raw OHLC unless you explicitly want them)\n",
    "FEATURE_COLS = [\n",
    "    # price dynamics\n",
    "    \"log_return_1h\",\n",
    "    \"vol_12h\", \"vol_24h\",\n",
    "    \"log_volume\", \"vol_z_24h\",\n",
    "\n",
    "    # sentiment / events\n",
    "    \"news_count\", \"has_news\",\n",
    "    \"overall_sentiment_ffill\",\n",
    "    \"ticker_sentiment_ffill\",\n",
    "    \"ticker_relevance_ffill\",\n",
    "\n",
    "    # macro regime (already scaled)\n",
    "    \"CPI_YoY\", \"CPI_MoM\",\n",
    "    \"PPI_YoY\",\n",
    "    \"GDP_Growth_QoQ\",\n",
    "    \"Fed_Funds_Rate\",\n",
    "    \"NonFarm_Payrolls_Change\",\n",
    "]\n",
    "\n",
    "TARGET_COL = \"target_log_return_4h\"\n",
    "\n",
    "missing = [c for c in FEATURE_COLS + [TARGET_COL, \"ticker\", \"Datetime\", \"split\"] if c not in prices_all.columns]\n",
    "print(\"Missing columns:\", missing)\n",
    "\n",
    "assert not missing, f\"Missing required columns: {missing}\"\n",
    "\n",
    "print(\"Feature count:\", len(FEATURE_COLS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e47bca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all shape: (87479, 24, 16)\n",
      "y_all shape: (87479,)\n",
      "meta_all shape: (87479, 3)\n",
      "\n",
      "Sequence counts:\n",
      "train: 61194 val: 13146 test: 13139\n",
      "\n",
      "Meta sample (Datetime, ticker, split):\n",
      "[[Timestamp('2023-12-09 12:30:00+0000', tz='UTC') 'AAPL' 'train']\n",
      " [Timestamp('2023-12-09 13:30:00+0000', tz='UTC') 'AAPL' 'train']\n",
      " [Timestamp('2023-12-09 14:30:00+0000', tz='UTC') 'AAPL' 'train']\n",
      " [Timestamp('2023-12-09 15:30:00+0000', tz='UTC') 'AAPL' 'train']\n",
      " [Timestamp('2023-12-09 16:30:00+0000', tz='UTC') 'AAPL' 'train']\n",
      " [Timestamp('2023-12-09 17:30:00+0000', tz='UTC') 'AAPL' 'train']\n",
      " [Timestamp('2023-12-09 18:30:00+0000', tz='UTC') 'AAPL' 'train']\n",
      " [Timestamp('2023-12-11 14:30:00+0000', tz='UTC') 'AAPL' 'train']\n",
      " [Timestamp('2023-12-11 15:30:00+0000', tz='UTC') 'AAPL' 'train']\n",
      " [Timestamp('2023-12-11 16:30:00+0000', tz='UTC') 'AAPL' 'train']]\n"
     ]
    }
   ],
   "source": [
    "LOOKBACK = 24  # you can later test 48, 72, etc.\n",
    "\n",
    "def build_sequences_panel(df, feature_cols, target_col, lookback):\n",
    "    \"\"\"\n",
    "    Build sequences per ticker, preserving temporal order.\n",
    "    The sequence label y corresponds to the row at the END of the window.\n",
    "    \"\"\"\n",
    "    X, y, meta = [], [], []  # meta will store (Datetime, ticker, split) for auditing\n",
    "\n",
    "    for ticker, g in df.groupby(\"ticker\", sort=False):\n",
    "        g = g.sort_values(\"Datetime\").reset_index(drop=True)\n",
    "\n",
    "        feat = g[feature_cols].to_numpy(dtype=np.float32)\n",
    "        targ = g[target_col].to_numpy(dtype=np.float32)\n",
    "        dt   = g[\"Datetime\"].to_numpy()\n",
    "        spl  = g[\"split\"].to_numpy()\n",
    "\n",
    "        n = len(g)\n",
    "        if n < lookback:\n",
    "            continue\n",
    "\n",
    "        for end in range(lookback - 1, n):\n",
    "            start = end - lookback + 1\n",
    "\n",
    "            X.append(feat[start:end+1, :])\n",
    "            y.append(targ[end])\n",
    "\n",
    "            meta.append((dt[end], ticker, spl[end]))\n",
    "\n",
    "    X = np.stack(X)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    meta = np.array(meta, dtype=object)\n",
    "\n",
    "    return X, y, meta\n",
    "\n",
    "X_all, y_all, meta_all = build_sequences_panel(\n",
    "    prices_all, FEATURE_COLS, TARGET_COL, LOOKBACK\n",
    ")\n",
    "\n",
    "print(\"X_all shape:\", X_all.shape)  # (num_samples, lookback, num_features)\n",
    "print(\"y_all shape:\", y_all.shape)\n",
    "print(\"meta_all shape:\", meta_all.shape)\n",
    "\n",
    "# Split by meta (end-of-window split)\n",
    "splits = meta_all[:, 2]\n",
    "train_mask = splits == \"train\"\n",
    "val_mask   = splits == \"val\"\n",
    "test_mask  = splits == \"test\"\n",
    "\n",
    "X_train, y_train = X_all[train_mask], y_all[train_mask]\n",
    "X_val, y_val     = X_all[val_mask], y_all[val_mask]\n",
    "X_test, y_test   = X_all[test_mask], y_all[test_mask]\n",
    "\n",
    "print(\"\\nSequence counts:\")\n",
    "print(\"train:\", X_train.shape[0], \"val:\", X_val.shape[0], \"test:\", X_test.shape[0])\n",
    "\n",
    "# Quick audit: show first few meta rows\n",
    "print(\"\\nMeta sample (Datetime, ticker, split):\")\n",
    "print(meta_all[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e4e60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # Ensure float32 tensors\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)  # (N, 1) for regression\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97c3aa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 239\n",
      "Val batches: 52\n",
      "Test batches: 52\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "train_ds = SequenceDataset(X_train, y_train)\n",
    "val_ds   = SequenceDataset(X_val, y_val)\n",
    "test_ds  = SequenceDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches:\", len(val_loader))\n",
    "print(\"Test batches:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e3ba6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X shape: torch.Size([256, 24, 16])\n",
      "Batch y shape: torch.Size([256, 1])\n",
      "X dtype: torch.float32 y dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch X shape:\", xb.shape)  # expected (B, 24, 16)\n",
    "print(\"Batch y shape:\", yb.shape)  # expected (B, 1)\n",
    "print(\"X dtype:\", xb.dtype, \"y dtype:\", yb.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "560d4aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature order: ['log_return_1h', 'vol_12h', 'vol_24h', 'log_volume', 'vol_z_24h', 'news_count', 'has_news', 'overall_sentiment_ffill', 'ticker_sentiment_ffill', 'ticker_relevance_ffill', 'CPI_YoY', 'CPI_MoM', 'PPI_YoY', 'GDP_Growth_QoQ', 'Fed_Funds_Rate', 'NonFarm_Payrolls_Change']\n"
     ]
    }
   ],
   "source": [
    "feature_order = FEATURE_COLS.copy()\n",
    "print(\"Feature order:\", feature_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afde652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # Ensure float32 tensors\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)  # (N, 1) for regression\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7213df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 239\n",
      "Val batches: 52\n",
      "Test batches: 52\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "train_ds = SequenceDataset(X_train, y_train)\n",
    "val_ds   = SequenceDataset(X_val, y_val)\n",
    "test_ds  = SequenceDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches:\", len(val_loader))\n",
    "print(\"Test batches:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e04a532e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X shape: torch.Size([256, 24, 16])\n",
      "Batch y shape: torch.Size([256, 1])\n",
      "X dtype: torch.float32 y dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch X shape:\", xb.shape)  # expected (B, 24, 16)\n",
    "print(\"Batch y shape:\", yb.shape)  # expected (B, 1)\n",
    "print(\"X dtype:\", xb.dtype, \"y dtype:\", yb.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff71ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, n_features, hidden_size=64, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # If num_layers == 1, PyTorch ignores dropout in LSTM, so handle gracefully\n",
    "        lstm_dropout = dropout if num_layers > 1 else 0.0\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=lstm_dropout\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        # hn: (num_layers, B, hidden_size) -> take last layer\n",
    "        h_last = hn[-1]  # (B, hidden_size)\n",
    "        yhat = self.head(h_last)  # (B, 1)\n",
    "        return yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e0ce156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "LSTMRegressor(\n",
      "  (lstm): LSTM(16, 64, batch_first=True)\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "model = LSTMRegressor(\n",
    "    n_features=n_features,\n",
    "    hidden_size=64,\n",
    "    num_layers=1,\n",
    "    dropout=0.0\n",
    ").to(device)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54cf9ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([256, 1])\n",
      "yb shape: torch.Size([256, 1])\n",
      "pred finite: True\n",
      "pred sample: [-0.01507325 -0.03253574 -0.0279959  -0.03488839 -0.02618472]\n"
     ]
    }
   ],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "xb = xb.to(device)\n",
    "yb = yb.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(xb)\n",
    "\n",
    "print(\"pred shape:\", pred.shape)\n",
    "print(\"yb shape:\", yb.shape)\n",
    "\n",
    "# sanity: should be finite\n",
    "print(\"pred finite:\", torch.isfinite(pred).all().item())\n",
    "print(\"pred sample:\", pred[:5].view(-1).cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74d46478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def rmse_from_mse(mse):\n",
    "    return math.sqrt(mse)\n",
    "\n",
    "def run_one_epoch(model, loader, optimizer=None, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    If optimizer is provided -> train mode (backprop).\n",
    "    Else -> eval mode (no grad).\n",
    "    Returns average MSE and RMSE.\n",
    "    \"\"\"\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "\n",
    "    total_mse = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            # Optional: gradient clipping for stability (good for LSTMs)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        total_mse += loss.item() * bs\n",
    "        n += bs\n",
    "\n",
    "    avg_mse = total_mse / n\n",
    "    return avg_mse, rmse_from_mse(avg_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "192d11ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train RMSE=0.010045 | val RMSE=0.015472\n",
      "Epoch 02 | train RMSE=0.009530 | val RMSE=0.011719\n",
      "Epoch 03 | train RMSE=0.009488 | val RMSE=0.010813\n",
      "Epoch 04 | train RMSE=0.009436 | val RMSE=0.011997\n",
      "Epoch 05 | train RMSE=0.009410 | val RMSE=0.016487\n",
      "Epoch 06 | train RMSE=0.009408 | val RMSE=0.009695\n",
      "Epoch 07 | train RMSE=0.009360 | val RMSE=0.010491\n",
      "Epoch 08 | train RMSE=0.009332 | val RMSE=0.009478\n",
      "Epoch 09 | train RMSE=0.009338 | val RMSE=0.008548\n",
      "Epoch 10 | train RMSE=0.009313 | val RMSE=0.010093\n",
      "Epoch 11 | train RMSE=0.009288 | val RMSE=0.015195\n",
      "Epoch 12 | train RMSE=0.009279 | val RMSE=0.008012\n",
      "Epoch 13 | train RMSE=0.009227 | val RMSE=0.009376\n",
      "Epoch 14 | train RMSE=0.009204 | val RMSE=0.009270\n",
      "Epoch 15 | train RMSE=0.009200 | val RMSE=0.010687\n",
      "Epoch 16 | train RMSE=0.009174 | val RMSE=0.007975\n",
      "Epoch 17 | train RMSE=0.009104 | val RMSE=0.008742\n",
      "Epoch 18 | train RMSE=0.009093 | val RMSE=0.009372\n",
      "Epoch 19 | train RMSE=0.009039 | val RMSE=0.010277\n",
      "Epoch 20 | train RMSE=0.009049 | val RMSE=0.010254\n",
      "Epoch 21 | train RMSE=0.008984 | val RMSE=0.010618\n",
      "Early stopping triggered at epoch 21. Best val RMSE=0.007975\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Hyperparameters (baseline)\n",
    "LR = 1e-3\n",
    "EPOCHS = 30\n",
    "PATIENCE = 5  # early stop if val doesn't improve for this many epochs\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "best_val_mse = float(\"inf\")\n",
    "best_state = None\n",
    "pat = 0\n",
    "\n",
    "history = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_mse, train_rmse = run_one_epoch(model, train_loader, optimizer=optimizer, device=device)\n",
    "    val_mse, val_rmse     = run_one_epoch(model, val_loader, optimizer=None, device=device)\n",
    "\n",
    "    history.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_mse\": train_mse, \"train_rmse\": train_rmse,\n",
    "        \"val_mse\": val_mse,     \"val_rmse\": val_rmse\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"train RMSE={train_rmse:.6f} | val RMSE={val_rmse:.6f}\"\n",
    "    )\n",
    "\n",
    "    # Early stopping on val MSE\n",
    "    if val_mse < best_val_mse - 1e-10:\n",
    "        best_val_mse = val_mse\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        pat = 0\n",
    "    else:\n",
    "        pat += 1\n",
    "        if pat >= PATIENCE:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}. Best val RMSE={math.sqrt(best_val_mse):.6f}\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c752d6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final TEST RMSE: 0.008293\n"
     ]
    }
   ],
   "source": [
    "test_mse, test_rmse = run_one_epoch(model, test_loader, optimizer=None, device=device)\n",
    "print(f\"\\nFinal TEST RMSE: {test_rmse:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fba024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 13139\n",
      "X_last shape: (13139, 16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Rebuild meta masks from earlier variables\n",
    "splits = meta_all[:, 2]\n",
    "test_mask = splits == \"test\"\n",
    "\n",
    "# True y for test\n",
    "y_true = y_all[test_mask].astype(np.float64)\n",
    "\n",
    "# Extract last-step features for baselines (timestep = -1)\n",
    "X_last = X_all[test_mask, -1, :].astype(np.float64)  # shape: (N_test, F)\n",
    "\n",
    "# Feature index helpers\n",
    "feat_index = {f:i for i,f in enumerate(FEATURE_COLS)}\n",
    "idx_lr1h = feat_index[\"log_return_1h\"]\n",
    "\n",
    "print(\"Test samples:\", len(y_true))\n",
    "print(\"X_last shape:\", X_last.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d391f69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (predict 0) TEST RMSE: 0.007579834704195661\n"
     ]
    }
   ],
   "source": [
    "y_pred_zero = np.zeros_like(y_true)\n",
    "rmse_zero = np.sqrt(mean_squared_error(y_true, y_pred_zero))\n",
    "print(\"Baseline (predict 0) TEST RMSE:\", rmse_zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cac8f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (predict log_return_1h) TEST RMSE: 0.00888768808856586\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr1h = X_last[:, idx_lr1h]\n",
    "rmse_lr1h = np.sqrt(mean_squared_error(y_true, y_pred_lr1h))\n",
    "print(\"Baseline (predict log_return_1h) TEST RMSE:\", rmse_lr1h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "741cd7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge alpha=0.1    val RMSE=0.007399\n",
      "Ridge alpha=1.0    val RMSE=0.007399\n",
      "Ridge alpha=10.0   val RMSE=0.007399\n",
      "Ridge alpha=100.0  val RMSE=0.007398\n",
      "\n",
      "Best Ridge alpha: 100.0 val RMSE: 0.007397902718953849\n",
      "Baseline (Ridge last-step) TEST RMSE: 0.007598440937625124\n"
     ]
    }
   ],
   "source": [
    "# Train/val split masks for fitting ridge properly\n",
    "train_mask = splits == \"train\"\n",
    "val_mask   = splits == \"val\"\n",
    "\n",
    "X_train_last = X_all[train_mask, -1, :].astype(np.float64)\n",
    "y_train_last = y_all[train_mask].astype(np.float64)\n",
    "\n",
    "X_val_last = X_all[val_mask, -1, :].astype(np.float64)\n",
    "y_val_last = y_all[val_mask].astype(np.float64)\n",
    "\n",
    "# Small alpha grid, choose best on val\n",
    "alphas = [0.1, 1.0, 10.0, 100.0]\n",
    "best = None\n",
    "\n",
    "for a in alphas:\n",
    "    reg = Ridge(alpha=a, random_state=42)\n",
    "    reg.fit(X_train_last, y_train_last)\n",
    "    val_pred = reg.predict(X_val_last)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val_last, val_pred))\n",
    "    print(f\"Ridge alpha={a:<6} val RMSE={val_rmse:.6f}\")\n",
    "    if best is None or val_rmse < best[\"val_rmse\"]:\n",
    "        best = {\"alpha\": a, \"model\": reg, \"val_rmse\": val_rmse}\n",
    "\n",
    "# Evaluate best ridge on test\n",
    "ridge_best = best[\"model\"]\n",
    "y_pred_ridge = ridge_best.predict(X_last)\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_true, y_pred_ridge))\n",
    "\n",
    "print(\"\\nBest Ridge alpha:\", best[\"alpha\"], \"val RMSE:\", best[\"val_rmse\"])\n",
    "print(\"Baseline (Ridge last-step) TEST RMSE:\", rmse_ridge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea46149f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                model  test_rmse\n",
      "0                Zero   0.007580\n",
      "2    Ridge(last-step)   0.007598\n",
      "3      LSTM(sequence)   0.008293\n",
      "1  Last log_return_1h   0.008888\n"
     ]
    }
   ],
   "source": [
    "rmse_lstm = 0.008293  # <-- your reported test RMSE\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"model\": [\"Zero\", \"Last log_return_1h\", \"Ridge(last-step)\", \"LSTM(sequence)\"],\n",
    "    \"test_rmse\": [rmse_zero, rmse_lr1h, rmse_ridge, rmse_lstm]\n",
    "}).sort_values(\"test_rmse\")\n",
    "\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VScodeEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
